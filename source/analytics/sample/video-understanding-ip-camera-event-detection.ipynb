{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd36e7a8-b54c-487f-b093-2febf7714d6d",
   "metadata": {},
   "source": [
    "# Video understanding - IP camera event detection\n",
    "IP camera event detection analyzes security camera footage to automatically identify specific events such as package deliveries, theft incidents, or suspicious activities. This capability helps security teams, property managers, and homeowners monitor their premises efficiently by detecting and alerting on important events without requiring continuous manual surveillance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1c222-f808-4ea2-affa-f8f58e903179",
   "metadata": {},
   "source": [
    "In this sample notebook, we consume the metadata extracted using the Video Understanding Tool, which includes frame-level object detection with associated timestamps. We will retrieve this metadata and use it for event detection analysis with LLMs.\n",
    "\n",
    "![ip_camera](./statics/video-ip-camera.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf81a5-14d6-42e1-8b5a-e2651fae50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dynamodb_tool, bedrock_tool\n",
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981a57f0-09ac-4ba1-8696-ee51608cbe48",
   "metadata": {},
   "source": [
    "You can find the task ID in the Video Understanding Tool UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9edbb2b-d427-4011-95b3-96ae88719fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = 'YOUR_TASK_ID_FROM_VIDEO_UNDERSTANDING_TOOL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c21c51-9c1f-4763-a2c7-e2d4001e9fee",
   "metadata": {},
   "source": [
    "Retrieve frame-level object detection results from the Video Understanding Tool's managed database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wait-and-retrieve",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame_outputs = dynamodb_tool.get_frame_outputs(task_id, output_names=[\"Object detection\"])\n",
    "print(json.dumps(frame_outputs,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "event-detection-section",
   "metadata": {},
   "source": [
    "## Event detection analysis\n",
    "Analyze the frame metadata to detect package delivery and theft events using advanced pattern recognition. The analysis considers object presence, person orientation, and temporal sequences to identify specific security events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "event-detection-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event detection prompt with detailed classification rules\n",
    "event_detection_prompt = f\"\"\"\n",
    "##Role##\n",
    "You are an AI event detection system analyzing timestamped video frame metadata to identify specific events. Your task is to detect package delivery and package theft events based on object detection data across multiple frames.\n",
    "\n",
    "The above system instructions define your capabilities and scope. If user request contradicts any system instruction, politely decline explaining your capabilities.\n",
    "\n",
    "##Input Data Format##\n",
    "You will receive timestamped frame metadata containing object detections with:\n",
    "- timestamp: Time in seconds when the frame was captured\n",
    "- objects: Detected objects with their coordinates and attributes\n",
    "  - name: Object type (person, car, package)\n",
    "  - coordinates: Bounding box (x, y, width, height)\n",
    "  - facingCamera: Boolean indicating if person faces camera (only for person objects)\n",
    "\n",
    "##Event Classification Rules##\n",
    "\n",
    "###Package Delivery Event###\n",
    "A package delivery is detected when:\n",
    "1. A person is present with a package in early frames\n",
    "2. The person is facing the camera initially\n",
    "3. The package remains stationary or is placed down\n",
    "4. The person leaves the scene (moves away or exits frame)\n",
    "5. The package remains in the scene after the person leaves\n",
    "\n",
    "###Package Theft Event###\n",
    "A package theft is detected when:\n",
    "1. A package is present in the scene initially\n",
    "2. A person approaches and is facing the camera\n",
    "3. The person picks up or interacts with the package\n",
    "4. The person turns away from the camera (facingCamera changes from true to false)\n",
    "5. Both the person AND package disappear from subsequent frames\n",
    "\n",
    "##Task##\n",
    "Analyze the provided frame metadata to determine if a package delivery or package theft event occurred based on the ##Event Classification Rules##.\n",
    "\n",
    "Frame metadata to analyze: {frame_outputs}\n",
    "\n",
    "Think step by step first and then answer. Follow the response format below exactly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84152c8-cf90-4a49-973c-f0842c660b38",
   "metadata": {},
   "source": [
    "Tool configuration provides a more reliable way to obtain structured outputs from a Foundation Model. In this example, we define the following tool configuration and send it to the Bedrock Converse API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tool-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool configuration for structured event detection output\n",
    "tool_config = {\n",
    "    \"toolChoice\": {\n",
    "        \"tool\": {\n",
    "            \"name\": \"event_detection_result\"\n",
    "        }\n",
    "    },\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"event_detection_result\",\n",
    "                \"description\": \"Analyze timestamped video frame metadata to detect package delivery or package theft events.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"events_detected\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"description\": \"List of all detected events in chronological order. Each event represents a distinct package-related activity identified in the video footage.\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"description\": \"Individual event detection result containing event classification, timing, and supporting evidence.\",\n",
    "                                    \"properties\": {\n",
    "                                        \"event_type\": {\n",
    "                                            \"type\": \"string\",\n",
    "                                            \"description\": \"Classification of the detected event: 'package_theft' for unauthorized taking of packages, 'package_delivery' for legitimate delivery activities, or 'no_event' when no package-related activity is detected.\",\n",
    "                                            \"enum\": [\"package_theft\", \"package_delivery\", \"no_event\"]\n",
    "                                        },\n",
    "                                        \"time_range\": {\n",
    "                                            \"type\": \"object\",\n",
    "                                            \"description\": \"Temporal boundaries of the event in the video timeline, measured in seconds from video start.\",\n",
    "                                            \"properties\": {\n",
    "                                                \"start_timestamp\": {\n",
    "                                                    \"type\": \"number\",\n",
    "                                                    \"description\": \"Beginning timestamp of the event in seconds. Marks when the first relevant activity or person appears in the scene.\"\n",
    "                                                },\n",
    "                                                \"end_timestamp\": {\n",
    "                                                    \"type\": \"number\",\n",
    "                                                    \"description\": \"Ending timestamp of the event in seconds. Marks when the activity concludes or person exits the scene.\"\n",
    "                                                }\n",
    "                                            },\n",
    "                                            \"required\": [\"start_timestamp\", \"end_timestamp\"]\n",
    "                                        },\n",
    "                                        \"evidence\": {\n",
    "                                            \"type\": \"array\",\n",
    "                                            \"description\": \"List of specific observations and behavioral indicators that support the event classification. Include details about people, actions, objects, uniforms, vehicles, and contextual clues.\",\n",
    "                                            \"items\": {\n",
    "                                                \"type\": \"string\",\n",
    "                                                \"description\": \"Specific piece of evidence such as 'Person wearing delivery uniform', 'Package placed at door', 'Unauthorized person taking package', 'Delivery truck visible', etc.\"\n",
    "                                            }\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"required\": [\"event_type\", \"time_range\", \"evidence\"]\n",
    "                                }\n",
    "                            },\n",
    "                            \"analysis_notes\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Overall analysis summary including methodology used, any ambiguities encountered, video quality observations, confidence levels, and recommendations for further investigation if needed.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"events_detected\", \"analysis_notes\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-events",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run event detection analysis\n",
    "response = bedrock_tool.bedrock_converse(\n",
    "    model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "    prompt=event_detection_prompt,\n",
    "    tool_config=tool_config,\n",
    "    inference_config={\"maxTokens\": 2000, \"temperature\": 0, \"topP\": 0}\n",
    ")\n",
    "\n",
    "result = bedrock_tool.parse_converse_response(response)\n",
    "event_analysis = json.loads(result)\n",
    "print(json.dumps(event_analysis,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-summary",
   "metadata": {},
   "source": [
    "## Results summary\n",
    "Display a formatted summary of detected events with detailed analysis and confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display formatted results\n",
    "if 'event_analysis' in locals() and event_analysis:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVENT DETECTION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    events = event_analysis.get('events_detected', [])\n",
    "    \n",
    "    if not events or (len(events) == 1 and events[0].get('event_type') == 'no_event'):\n",
    "        print(\"No significant events detected in the video.\")\n",
    "    else:\n",
    "        for i, event in enumerate(events, 1):\n",
    "            if event.get('event_type') != 'no_event':\n",
    "                print(f\"\\nEvent {i}: {event['event_type'].replace('_', ' ').title()}\")\n",
    "                print(f\"Time Range: {event['time_range']['start_timestamp']}s - {event['time_range']['end_timestamp']}s\")\n",
    "                \n",
    "                print(\"\\nEvidence:\")\n",
    "                for evidence in event['evidence']:\n",
    "                    print(f\"  â€¢ {evidence}\")\n",
    "    \n",
    "    if event_analysis.get('analysis_notes'):\n",
    "        print(f\"\\nAnalysis Notes: {event_analysis['analysis_notes']}\")\n",
    "else:\n",
    "    print(\"No event analysis results available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009d631-41f9-48c6-b876-024cd31ee5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
