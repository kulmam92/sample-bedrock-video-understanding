{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57475d4a-d44d-4ba9-9bc5-f2fc4422f870",
   "metadata": {},
   "source": [
    "# Video understanding - shot based chapter analysis\n",
    "Chapter analysis breaks a video into structured segments, summarizing key audio and visual content for each segment. In the media industry, this helps editors, producers, and analysts quickly understand the storyline, highlight important moments, and streamline content indexing for TV shows, films, documentaries, and news programs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa36ba-d2d1-4059-9d02-e1527285ac52",
   "metadata": {},
   "source": [
    "In this sample notebook, we consume the metadata extracted using the Video Understanding tool, which includes shot summaries and audio transcripts with associated timestamps. We will retrieve this metadata and use it for further chapter analysis with LLMs.\n",
    "\n",
    "![media_chapter](./statics/video-media-chapter.png)\n",
    "\n",
    "We will use [Meridian](https://en.wikipedia.org/wiki/Meridian_(film)), an open-source movie, for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8baa0-22a9-419e-9bb7-625540adcbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dynamodb_tool, bedrock_tool\n",
    "import json\n",
    "import boto3\n",
    "from IPython.display import JSON, Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261e1f34-492f-4f19-9f87-8e6ddb0a5d66",
   "metadata": {},
   "source": [
    "The results of the video understanding tool are stored in both Amazon S3 and Amazon DynamoDB, and you can access them directly within the same AWS account.\n",
    "\n",
    "In this lab, we will retrieve the extracted video summary and audio transcription from DynamoDB using the `dynamodb_utils` functions. The tool manages four DynamoDB tables:\n",
    "\n",
    "- Task table: **bedrock_mm_extr_srv_video_task** — Stores metadata related to video processing tasks, such as task ID, status, creation time, and the original request.\n",
    "- Frame table: **bedrock_mm_extr_srv_video_frame** — Stores frames associated with task IDs, containing frame-level analysis results generated by the frame-based pipeline.\n",
    "- Shot table: **bedrock_mm_extr_srv_video_shot** — Stores shot-level extraction results associated with task IDs, populated by the shot-based pipeline.\n",
    "- Transcript table: **bedrock_mm_extr_srv_video_transcript** — Stores audio transcriptions associated with each task ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7577acc-f989-41b2-8dea-6c66769ef432",
   "metadata": {},
   "source": [
    "You can find the task ID in the Video Understanding Tool UI.\n",
    "\n",
    "Go to the Shot-based section and select the Meridan video that was processed earlier using the UI. Click the Get Task ID link then replace the value below with it.\n",
    "\n",
    "![Find task Id](./statics/find-task-id.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b2a33-de7d-40e6-84ed-93e3080f1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = 'YOUR_TASK_ID_FROM_VIDEO_UNDERSTANDING_TOOL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b379702-6e26-4e81-842e-b36d6391ffcd",
   "metadata": {},
   "source": [
    "Retrieve transcripts from the Video Understanding Tool's managed database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a4ba9-cb02-4bb6-9878-226fc0919e2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcripts = dynamodb_tool.get_transcripts(task_id)\n",
    "print(json.dumps(transcripts,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c21c51-9c1f-4763-a2c7-e2d4001e9fee",
   "metadata": {},
   "source": [
    "Retrieve shot summaries from the Video Understanding Tool's managed database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452312c-90fd-428e-b148-f56183996f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shots = dynamodb_tool.get_shot_outputs(task_id=task_id, output_names=[\"Summarize shot\"])\n",
    "print(json.dumps(shots,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301cf3c-811a-474f-92ac-591fdd9024be",
   "metadata": {},
   "source": [
    "## Directly analyze chapters using both the transcripts and shot summaries\n",
    "For shorter videos (e.g., under 10 minutes), you can pass the audio transcripts and shot summaries directly to an LLM to analyze chapters.\n",
    "\n",
    "In this example, we use **Nova Pro**, which provides balanced reasoning capabilities. This model is well-suited for handling large input sizes (for longer videos) and performing reasoning tasks such as aligning timestamps between shot summaries and audio transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4eddc2-4a3a-4496-b81d-204288569923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reduce processing time for demo, we only use the first 30 shots as input\n",
    "shots_subset = shots[0:30] if len(shots) > 30 else shots\n",
    "\n",
    "prompt = f'''\n",
    "You are a media expert that summarizes videos into chapter-based segments. \n",
    "A chapter is a coherent segment of a video that groups together related content, events, or scenes. Each chapter has a defined start and end time and can include both audio and visual information, providing a summarized view of that portion of the video.\n",
    "You are given two inputs:\n",
    "Shot-level visual descriptions with timestamps.\n",
    "Audio transcription text with timestamps.\n",
    "Your task is to:\n",
    "Merge the visual and audio information to identify meaningful chapters.\n",
    "Group adjacent frames and transcript segments into coherent chapters.\n",
    "For each chapter, provide:\n",
    "Start time (earliest timestamp from shot start_time/transcript).\n",
    "End time (latest timestamp from shot end_time/transcript).\n",
    "chapter summary (a concise description combining visual and audio context).\n",
    "Ensure chapter boundaries reflect major changes in visuals, topics, or conversations.\n",
    "Output the results as in the markdown format.\n",
    "\n",
    "Shot Summary:\n",
    "{shots_subset}\n",
    "Audio Transcript:\n",
    "{transcripts}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879f6ba-fe65-46bf-b24a-a1ee6e2e1edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = bedrock_tool.bedrock_converse(\n",
    "    model_id=\"us.amazon.nova-pro-v1:0\", \n",
    "    prompt=prompt, \n",
    "    inference_config={\n",
    "      \"maxTokens\": 10000,\n",
    "      \"topP\": 0.1,\n",
    "      \"temperature\": 0.3\n",
    "    }\n",
    ")\n",
    "result = bedrock_tool.parse_converse_response(response)\n",
    "display(Markdown(result.replace(\"\\\\n\", \"\\n\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1496d66-eb51-4008-b798-d1bd5a9d5135",
   "metadata": {},
   "source": [
    "## Chatper analysis for long form videos\n",
    "For longer media, such as a two-hour film with dense shot switches and extensive transcripts, providing all this information may exceed the LLM’s context window, and larger inputs often lead to less accurate results. A good optimization is to first summarize the audio transcripts into chapters, then use these chapter boundaries to group the visual (shot summaries) for a final chapter-level summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea71579-6313-4632-bc5e-51ffbd3e27f2",
   "metadata": {},
   "source": [
    "Deciding between audio-based or vision-based chapters depends on your video content. For most epsodical videos—such as TV shows, films, documentaries, sports, or news—that are professionally edited and where audio carries much of the information, audio-based chapters are generally more suitable as the baseline, with visual information blended in for additional context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5334a-8ad7-49c4-8f1f-7ea902c81d3b",
   "metadata": {},
   "source": [
    "### Summarize the audio transcript into chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f415b4-3198-40b3-a570-54be889a362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''\n",
    "You are an media expert that summarizes video audio transcripts into coherent, non-overlapping chapters. Your task is to analyze the transcript and segment the video into chapters with clear titles, summaries and time ranges.\n",
    "Guidelines:\n",
    "- Chapters must fully cover the video duration without gaps. The first chatper should start from 0 second.\n",
    "- For time ranges with no audio transcript available, create a default chapter (e.g., \"No Audio Transcript\") spanning that duration.\n",
    "- Chapters must be sequential and continuous, with the end time of one chapter matching the start time of the next.\n",
    "Audio Transcript:\n",
    "{transcripts}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84152c8-cf90-4a49-973c-f0842c660b38",
   "metadata": {},
   "source": [
    "Tool configuration provides a more reliable way to obtain structured outputs from a Foundation Model. In this example, we define the following tool configuration and send it to the Bedrock Converse API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7881cd64-38d5-400f-8fc5-7182f4a9a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_config = {\n",
    "    \"toolChoice\": {\n",
    "        \"tool\": {\n",
    "            \"name\": \"audio_chapters\"\n",
    "        }\n",
    "    },\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"audio_chapters\",\n",
    "                \"description\": \"Analyze the input audio transcripts and segment the video into chapters with clear titles, summaries and time ranges.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"chapters\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"title\": {\n",
    "                                            \"type\": \"string\"\n",
    "                                        },\n",
    "                                        \"start_time\": {\n",
    "                                            \"type\": \"number\"\n",
    "                                        },\n",
    "                                        \"end_time\": {\n",
    "                                            \"type\": \"number\"\n",
    "                                        },\n",
    "                                        \"audio_summary\": {\n",
    "                                            \"type\": \"string\"\n",
    "                                        }\n",
    "                                    },\n",
    "                                    \"required\": [\n",
    "                                        \"title\",\n",
    "                                        \"start_time\",\n",
    "                                        \"end_time\",\n",
    "                                        \"audio_summary\"\n",
    "                                    ]\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"chapters\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e682572-3570-4bba-a8ce-00f29f1c17f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = bedrock_tool.bedrock_converse(\n",
    "    model_id=\"us.amazon.nova-pro-v1:0\", \n",
    "    prompt=prompt, \n",
    "    tool_config=tool_config,\n",
    "    inference_config={\n",
    "      \"maxTokens\": 10000,\n",
    "      \"topP\": 0.1,\n",
    "      \"temperature\": 0.3\n",
    "    }\n",
    ")\n",
    "\n",
    "result = bedrock_tool.parse_converse_response(response)\n",
    "print(json.dumps(json.loads(result),indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098ab2d-30bc-482f-a2ab-a7d688c56030",
   "metadata": {},
   "source": [
    "### Align shots to the audio chatpers \n",
    "To generate chapter summaries using both the audio transcripts and the visual (shot) summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d94a1-4c8d-40d6-9b32-bd3ddd1b03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = json.loads(result)\n",
    "for chapter in chapters.get(\"chapters\"):\n",
    "    print(f'Processing chatper: {chapter[\"title\"]} [{chapter.get(\"start_time\")}-{chapter.get(\"end_time\")}] s')\n",
    "\n",
    "    # Find overlappring shots\n",
    "    overlapping_shots = [\n",
    "        item for item in shots\n",
    "        if item[\"start_time\"] < chapter.get(\"end_time\") and item[\"end_time\"] > chapter.get(\"start_time\")\n",
    "    ]\n",
    "    overlapping_shots_sorted = sorted(overlapping_shots, key=lambda x: x[\"start_time\"])\n",
    "\n",
    "    # Generate chapter summary using audio transcript summary and the shot summaries\n",
    "    chatper_prompt = f'''\n",
    "    You are a media expert tasked with creating comprehensive chapter summaries for videos. Your job is to combine information from both the audio transcripts (chapter summaries) and the visual content (shot summaries) to produce a unified, coherent chapter summary that reflects the key events and details from the entire video.\n",
    "    Do not include any Markdown prefixes in the result. Describe the video directly, without starting the summary with leading phrases such as 'The video begins', 'the video opens with'\n",
    "    Audio transcript summary:\n",
    "    {chapter.get(\"audio_summary\")}\n",
    "    Vision shots with summary:\n",
    "    {overlapping_shots_sorted}\n",
    "    '''\n",
    "    response = bedrock_tool.bedrock_converse(\n",
    "        model_id=\"us.amazon.nova-pro-v1:0\", \n",
    "        prompt=chatper_prompt, \n",
    "        inference_config={\n",
    "          \"maxTokens\": 10000,\n",
    "          \"topP\": 0.1,\n",
    "          \"temperature\": 0.3\n",
    "        }\n",
    "    )\n",
    "    full_summary = bedrock_tool.parse_converse_response(response)\n",
    "    chapter[\"full_summary\"] = full_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47412bc5-e69d-4c41-acd6-652478efae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(chapters,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb474f27",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Video chapter analysis is a complex topic, and the methods and logic can vary depending on the video content type and business requirements. This sample demonstrates two representative approaches to analyzing your video based on metadata extracted using the video understanding tool, serving as a starting point. You can further extend and customize the analytics flow to tailor the results to your specific needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
